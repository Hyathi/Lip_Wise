{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pawansharmaaaa/Wav2Lip_Mod/blob/master/GFPGAN_Wav2Lip_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QPH1UIrzkRD"
      },
      "source": [
        "#Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnHM2wvud_g9"
      },
      "source": [
        "Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qciH4PsUazL_",
        "outputId": "f9825d47-b240-45af-d382-7416a9383e52"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3cShHJ9nTb_"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuLbKrST5etk"
      },
      "outputs": [],
      "source": [
        "!cp -ri \"/content/drive/MyDrive/Wav2Lip_Mod/TAudio.wav\" \"/content/drive/MyDrive/Wav2Lip_Mod/input_vid.mp4\" /content/Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgOmJn0-eD4c"
      },
      "source": [
        "Wav2Lip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlOPNDG-f6n9",
        "outputId": "f7455796-42f9-4905-fc07-5781695360b3"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pawansharmaaaa/Wav2Lip_Mod.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-TvJSOOgDuN"
      },
      "outputs": [],
      "source": [
        "!cp -ri \"/content/drive/MyDrive/Wav2Lip_Mod/wav2lip.pth\" \"/content/drive/MyDrive/Wav2Lip_Mod/wav2lip_gan.pth\" /content/Wav2Lip_Mod/checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ooh28vw-Uvd3",
        "outputId": "53817fd0-4a21-45f6-cfe7-35a29e6c48ec"
      },
      "outputs": [],
      "source": [
        "!pip uninstall tensorflow tensorflow-gpu\n",
        "!cd Wav2Lip_Mod && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Gp5rgX2gily",
        "outputId": "5c0ac10e-8d76-4467-b35c-091983a46bcd"
      },
      "outputs": [],
      "source": [
        "!wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O \"Wav2Lip_Mod/face_detection/detection/sfd/s3fd.pth\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg1Q8kdid7fa"
      },
      "source": [
        "GFPGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDLmcaR7znD9"
      },
      "outputs": [],
      "source": [
        "!cd Wav2Lip_Mod && git clone https://github.com/TencentARC/GFPGAN.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bkAWnWSzuD9"
      },
      "outputs": [],
      "source": [
        "!pip install basicsr facexlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F193o3k9z6ga"
      },
      "outputs": [],
      "source": [
        "!cd Wav2Lip_Mod/GFPGAN && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lksYC-4N1iqE"
      },
      "outputs": [],
      "source": [
        "!cd Wav2Lip_Mod/GFPGAN && python setup.py develop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOyZ7DAN0Ht8"
      },
      "outputs": [],
      "source": [
        "!pip install realesrgan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9z8IvMm0MPi"
      },
      "outputs": [],
      "source": [
        "!cd Wav2Lip_Mod/GFPGAN && wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQclq04B0WFH"
      },
      "outputs": [],
      "source": [
        "!cd Wav2Lip_Mod/GFPGAN && python inference_gfpgan.py -i /content/GFPGAN/inputs/test2.jpg -o results/new -v 1.3 -s 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZBKqSViD5_L"
      },
      "outputs": [],
      "source": [
        "!pip install gfpgan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONBjxcJX5PY9"
      },
      "outputs": [],
      "source": [
        "!cp -ri \"/content/Data/w2l_input_video.mp4\" /content/drive/MyDrive/Finalized_notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFbemEAJhui2"
      },
      "source": [
        "#*Remove Frames Which do not contain faces from the Video*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2WqC9RbJvU8"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import face_recognition\n",
        "import gc\n",
        "import tqdm\n",
        "\n",
        "# Function to detect face in a frame\n",
        "def detect_face(frames):\n",
        "    batch_face_locations = face_recognition.batch_face_locations(frames, number_of_times_to_upsample=0)\n",
        "    return batch_face_locations\n",
        "\n",
        "# Open the video file for reading\n",
        "video_path = '/content/Data/linus.mp4'\n",
        "video_capture = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video properties\n",
        "fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
        "frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Create video writers for face.mp4\n",
        "face_writer = cv2.VideoWriter('/content/Data/w2l_input_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
        "\n",
        "batch_size = 16\n",
        "frames = []\n",
        "while True:\n",
        "    ret, frame = video_capture.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frames.append(frame)\n",
        "\n",
        "    if len(frames) == batch_size:\n",
        "      batch_faces = detect_face(frames)\n",
        "\n",
        "      for frame_no, face_locations in enumerate(tqdm.tqdm(batch_faces, total=batch_size)):\n",
        "        if len(face_locations) == 1:\n",
        "            # Write the frame to input_video.mp4 if a face is detected\n",
        "            pos_frame = cv2.cvtColor(frames[frame_no], cv2.COLOR_RGB2BGR)\n",
        "            face_writer.write(pos_frame)\n",
        "      frames = []\n",
        "\n",
        "# Release video capture, writers and clear garbage\n",
        "video_capture.release()\n",
        "face_writer.release()\n",
        "cv2.destroyAllWindows()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT0FwwmPnltV"
      },
      "source": [
        "#Running Wav2Lip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_av1IIc4PWN"
      },
      "outputs": [],
      "source": [
        "!cd Wav2Lip_Mod && python inference_wgan.py --checkpoint_path checkpoints/wav2lip.pth --face \"/content/Data/w2l_input_video.mp4\" --audio \"/content/Data/TAudio.wav\" --resize_factor 2 --upscale --nosmooth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPqP6ZzJ_ueK"
      },
      "source": [
        "#FRAMER.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UV6K6eTFhOW"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/GFPGAN')\n",
        "!cd GFPGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF6L-bst_tz4"
      },
      "outputs": [],
      "source": [
        "!cd GFPGAN\n",
        "import cv2\n",
        "import os\n",
        "from gfpgan import GFPGANer\n",
        "import torch\n",
        "\n",
        "\n",
        "def divide_video_into_frames(video_path, output_dir):\n",
        "    #LOAD REALESRGAN\n",
        "    upsample_bg = 'realesrgan'\n",
        "    if upsample_bg == 'realesrgan':\n",
        "        if not torch.cuda.is_available():  # CPU\n",
        "            import warnings\n",
        "            warnings.warn('The unoptimized RealESRGAN is slow on CPU. We do not use it. '\n",
        "                          'If you really want to use it, please modify the corresponding codes.')\n",
        "            bg_upsampler = None\n",
        "        else:\n",
        "            from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "            from realesrgan import RealESRGANer\n",
        "            model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n",
        "            bg_upsampler = RealESRGANer(\n",
        "                scale=2,\n",
        "                model_path='https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth',\n",
        "                model=model,\n",
        "                tile=0,\n",
        "                tile_pad=10,\n",
        "                pre_pad=0,\n",
        "                half=True)  # need to set False in CPU mode\n",
        "    else:\n",
        "        bg_upsampler = None\n",
        "\n",
        "    #LOAD GFPGAN\n",
        "    arch = 'clean'\n",
        "    channel_multiplier = 2\n",
        "    model_name = 'GFPGANv1.3'\n",
        "    url = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth'\n",
        "\n",
        "    restorer = GFPGANer(\n",
        "        model_path=url,\n",
        "        upscale=2,\n",
        "        arch=arch,\n",
        "        channel_multiplier=channel_multiplier,\n",
        "        bg_upsampler=bg_upsampler)\n",
        "\n",
        "    # Get the video capture object\n",
        "    video_capture = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get the number of frames in the video\n",
        "    num_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(f\"Frames to improve: {num_frames}\")\n",
        "\n",
        "    # Iterate over the frames in the video\n",
        "    for i in range(num_frames):\n",
        "        # Read the frame from the video\n",
        "        success, frame = video_capture.read()\n",
        "\n",
        "        # If the frame was read successfully, save it\n",
        "        if success:\n",
        "            frame_name = os.path.join(output_dir, str(i) + \".jpg\")\n",
        "            cropped_faces, restored_faces, restored_img = restorer.enhance(\n",
        "              frame,\n",
        "              has_aligned=False,\n",
        "              only_center_face=False,\n",
        "              paste_back=True,\n",
        "              weight=0.5)\n",
        "\n",
        "            cv2.imwrite(frame_name, restored_img)\n",
        "\n",
        "    # Close the video capture object\n",
        "    video_capture.release()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set the video path and output directory\n",
        "    video_path = \"/content/result_voice_theek.mp4\"\n",
        "    output_dir = \"/content/Frames\"\n",
        "\n",
        "    # Divide the video into frames\n",
        "    divide_video_into_frames(video_path, output_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "eFbemEAJhui2",
        "IPqP6ZzJ_ueK"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
